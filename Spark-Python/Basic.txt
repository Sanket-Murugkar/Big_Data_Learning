pysqprk shell

............................................................

 a= sc.textFile("/home/vaibhav/dataset/EMP.csv");

 c= a.map(lambda m: m.split(",")).map(lambda m: (m[0],m[1]))

  c.collect()


..................................................


a= sc.textFile("/home/vaibhav/dataset/EMP.csv");

 c= a.map(lambda m: m.split(",")).map(lambda m: (m[1].upper()))

  c.collect

..................................................................



df= spark.read.json("/home/vaibhav/dataset/EMP1.json");

df.show();

df.collect();

df.dtypes


..........Loading CSV files.............

// We have to mount schema for this, on later version of pyspark we can use sc.read.json();

from pyspark.sql.types import *;

f= StructType(
[
StructField(("eid"),IntegerType()),
StructField(("first_name"),StringType()),
StructField(("last_name"),StringType()),
StructField(("salary"),IntegerType()),
StructField(("dId"),IntegerType())
]
)

m= (spark.read.schema(f).option("header","true").option("node","DROPMALFORMED").csv("/home/vaibhav/dataset/EMP_pyspark.csv")

m.show();

// To use sql queries
m.createTempView("EMP")

 sql("select * from EMP where eid>100").show()



...............................................................











